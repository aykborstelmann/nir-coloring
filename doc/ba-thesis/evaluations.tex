\chapter{Evaluation and Discussion}

\section{Evaluation Metrics}

As NIR colorization has two main application contexts: providing human users more interpretable images and to improve object recognition results by enriching the input with more information.
To assess the effectiveness of both models, we define qualitative evaluation categories and quantitative evaluation metrics based on their ability to achieve these goals.

\subsection{Quantitative Evaluation Metrics}
Due to the unpaired setting, in the quantitative evaluation classic solutions, such as the difference between the absolute intensity values or SSIM \parencite{ssim}, cannot be applied. Fortunately, methods for comparing the general image similarity of two sets, such as FID \parencite{ttur} or
comparing classification results using an off-the-shelf network, are well known to quantitatively assess the quality of image translation.

\subsubsection*{FID}
To measure how close the generated images are to the real images concerning human perception, the \textit{Fréchet Inception Distance} (FID) \parencite{ttur} is a commonly used metric.
It empirically estimates how the human eye perceives images for a set of images and computes the distance between two such set representations.
First, a pre-trained InceptionV3 model evaluates each image of the image set and the activation of the last layer is considered the "human perception approximation".
Then for all activation vectors of the evaluated images in the images set, a multidimensional Gaussian is fitted over those activation vectors.
This is done for two image sets, and later the two Gaussians are compared using the Fréchet distance \parencite{ttur}.
Intuitively, if the generated images are realistic, the statistics of features in a classification network should be similar to the real ones.

\subsubsection*{Evaluation of the FID}
\label{sec:evaluate-fid}

The FID is commonly used on a wide variety of unpaired image comparison tasks.

Conceptually it first, it is only a distance metric between the features of two image sets.
The features are extracted using


\subsection{Qualitative Evaluation Methods}
For qualitative evaluation, we define categories on how we determine the superior generated images. These are again based on the two main application contexts.

The first category describes the \textit{naturality} of an image, where the generated images should resemble how humans perceive the world, without artifacts or periodic patterns that can affect visual perception.

The second category refers to the \textit{content preservation} of the input image. This should help humans as well as object detection systems to find and classify animals accurately.

\textit{Hallucinations}, as artifacts or scene elements that are not in the input image, contradict naturality and content preservation, and therefore are undesirable.

\section{Loss-Guided and Correction-Guided Sampling}
Current approaches for unpaired image translation mostly suggest training an unconditional model on the target domain
and condition on the input images while sampling.
We find that methods for conditioning on an input image can be grouped into \textit{loss-guided} (\autoref{sec:energy-guided-sampling}) and \textit{correction-guided} (\autoref{sec:correction-guided-sampling}) sampling.

Generally when using the correction-guided approach, the current sampled image has to be decomposed into multiple parts,
one part being the one to be modified and the second one being the remainder.
This transformation of decomposition must also be invertible to create a function framework.
Not all problems or properties are suited for this.
Formulation as a loss is often easier to obtain and sometimes to only possibility.

In the case of near-infrared colorization both methods are actionable.
We evaluate for the context of near-infrared colorization which method should be used.
For both cases we condition using near-infrared images by assuming the intensity of the sample should be equal to the near-infrared image.
We discuss this particular conditioning method separately in \autoref{sec:nir-as-intensity-approximation-evaluation}.
In \autoref{fig:qualitative-evaluation-loss-guided-vs-correction-guided} we demonstrate the difference when formulating exactly the same property as loss-guided sampling and as correction-guided sampling.
It can be observed that the contours appear generally more noisy in the loss-guided sampling.
This observation is also quantitatively reflected, the FID of the loss-guided sampling is $23.64$ points higher than of the correction-guided approach (\autoref{fig:quantitative-evaluation-loss-guided-vs-correction-guided}).

\begin{figure}[htp!]
    \centering
    \setkeys{Gin}{width=1\linewidth}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
        \centering NIR                                                                                            & Correction-Guided                                                                                                                 & Loss-Guided                                                                                                                 & NIR                                                                                                       & Correction-Guided                                                                                                                 & Loss-Guided                                                                                                                 \\
        \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/nir_S2_B06_R1_PICT0128.jpg} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-correction-guided_S2_B06_R1_PICT0128.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-loss-guided_S2_B06_R1_PICT0128.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/nir_S2_B06_R1_PICT0279.jpg} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-correction-guided_S2_B06_R1_PICT0279.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-loss-guided_S2_B06_R1_PICT0279.png} \\
        \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/nir_S2_B06_R1_PICT0387.jpg} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-correction-guided_S2_B06_R1_PICT0387.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-loss-guided_S2_B06_R1_PICT0387.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/nir_S2_B06_R3_PICT1364.jpg} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-correction-guided_S2_B06_R3_PICT1364.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-loss-guided_S2_B06_R3_PICT1364.png} \\
        \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/nir_S2_B06_R3_PICT3848.jpg} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-correction-guided_S2_B06_R3_PICT3848.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-loss-guided_S2_B06_R3_PICT3848.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/nir_S2_B07_R1_PICT3274.jpg} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-correction-guided_S2_B07_R1_PICT3274.png} & \includegraphics{gfx/diffusion-sampling-loss-guided-vs-correction-guided-qual/diffusion-loss-guided_S2_B07_R1_PICT3274.png}
    \end{tabularx}
    \caption{
        \todo{Add Caption}
    }
    \label{fig:qualitative-evaluation-loss-guided-vs-correction-guided}
\end{figure}

\begin{table}[htp!]
    \centering
    \begin{tabular}{c | c}
        Model             & FID  $\downarrow$ \\
        \hline\hline
        Correction-Guided & 105.00            \\
        Loss-Guided       & 128.64
    \end{tabular}
    \caption{
        \todo{Add Caption}
    }
    \label{fig:quantitative-evaluation-loss-guided-vs-correction-guided}
\end{table}

\todo{Provide intuition for this observation !}
\todo{Unify with observations of \autoref{sec:high-pass-filter-evaluation}}

\section{Conditioning the Near-Infrared}
\subsection{NIR as Intensity Approximation}
\label{sec:nir-as-intensity-approximation-evaluation}
\Textcite{sbgm} already suggested colorization methods using diffusion models.
Although in comparison visible light, near-infrared light has different reflection properties,
we could ignore this property and assume the NIR intensity to be a good approximation of the visual light's intensity.

\begin{figure}[htp!]
    \centering
    \setkeys{Gin}{width=1\linewidth}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
        NIR                                                                                  & CycleGAN                                                                                       & Diffusion                                                                                  & NIR                                                                                  & CycleGAN                                                                                       & Diffusion                                                                                  \\
        \includegraphics{gfx/conditional-diffusion-sampling-qual/nir_S2_B06_R1_PICT0128.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-qual/cyclegan_S2_B06_R1_PICT0128_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/diffusion_S2_B06_R1_PICT0128.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/nir_S2_B06_R1_PICT0279.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-qual/cyclegan_S2_B06_R1_PICT0279_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/diffusion_S2_B06_R1_PICT0279.png} \\
        \includegraphics{gfx/conditional-diffusion-sampling-qual/nir_S2_B06_R1_PICT0387.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-qual/cyclegan_S2_B06_R1_PICT0387_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/diffusion_S2_B06_R1_PICT0387.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/nir_S2_B06_R3_PICT1364.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-qual/cyclegan_S2_B06_R3_PICT1364_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/diffusion_S2_B06_R3_PICT1364.png} \\
        \includegraphics{gfx/conditional-diffusion-sampling-qual/nir_S2_B06_R3_PICT3848.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-qual/cyclegan_S2_B06_R3_PICT3848_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/diffusion_S2_B06_R3_PICT3848.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/nir_S2_B07_R1_PICT3274.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-qual/cyclegan_S2_B07_R1_PICT3274_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-qual/diffusion_S2_B07_R1_PICT3274.png}
    \end{tabularx}
    \caption{
        \textbf{Qualitative Evaluation.} Left to right: Image from the NIR domain of the Serengeti test dataset \parencite{serengeti},
        corresponding image generated by CycleGAN \parencite{mehri} and corresponding sample generated by our diffusion model approach.
    }
    \label{fig:qualitative-evaluation-conditional-sampling}
\end{figure}

\todo{Explanation why approximation even works, night, few vegetation, ...}

Qualitatively we can also observe those generated images are faithful to the input image (\autoref{fig:qualitative-evaluation-conditional-sampling}).
It is to be mentioned that faithfulness to the input image is not a quality learned by the network, but a design choice of the sampling procedure.
Additionally, colors estimated by the diffusion model appear reasonable.
Weaknesses in the colorization are observable when analyzing the zebra image:
While CycleGAN colorizes the main zebra body as black and white, the diffusion network produces an orange-black zebra.
This can be traced back to a conceptional weakness.
The diffusion network is forced to reuse the intensity of the NIR image.
The zebra in the NIR image is gray-black and therefore the network is not capable in generating high intensity pixels.
CycleGAN on the other-hand is not restricted by design to change the intensity.
It is merely trained to produce invertible images.

\begin{table}[htp!]
    \centering
    \begin{tabular}{c | c}
        Model                                           & FID  $\downarrow$ \\
        \hline\hline
        CycleGAN                                        & 98.10             \\
        Unconditional Diffusion                         & 96.03             \\
        \textbf{Conditional Diffusion} \parencite{sbgm} & \textbf{105.00}
    \end{tabular}
    \caption{
        \textbf{Quantitative Evaluation.} CycleGAN and the diffusion model trained on Snapshot Serengeti \parencite{serengeti} containing only night NIR and RGB images.
        Comparing the FID calculated between the test dataset and the generated images.
    }
    \label{fig:quantitative-evaluation-conditional-sampling}
\end{table}


Similar to the minor qualitative weakness, we also observe the FID of this naive approach to be $6.9$ FID points worse than CycleGAN.
Additionally, we see a gap between the sampling the unconditional diffusion model produces our method according to \Citeauthor*{sbgm} of $8.97$ FID points \parencite{sbgm}.
This indicates that this method is too restrictive to allow competitive image generation.

To improve the intuition of the difference between the intensities, we consider CycleGAN's intensities a good approximation of the real visible intensity.
We compare this with the near-infrared intensity in \autoref{fig:heatmap-cycle-gan-intensity}.

\begin{figure}[htp!]
    \centering
    \includegraphics[width=\textwidth]{gfx/heatmap-nir-cycle-gan-intensity-diff.png}
    \caption{
        \todo{Better integration (using pdf or so); Best: Direct latex code}
        Absolute difference between CycleGAN's intensity and the NIR image.
        Left to Right: Intensity calculated by images generated by CycleGAN, Near-Infared image \parencite{serengeti} and difference between both.
    }
    \label{fig:heatmap-cycle-gan-intensity}
\end{figure}

We first observe the pattern, that CycleGAN does especially enlighten the foreground and with that the lower regions of the image.
The same holds for animals in the foreground such as the fox or zebra.
Indeed, while comparing the intensity of near-infrared and the incandescent image in the Serengeti dataset \parencite{serengeti}
this pattern is observable again:
Generally the light in incandescent images appears to be stronger and therefore foreground objects are more illuminated.

Naturally when fixing the intensity of the NIR image for the diffusion sampling, such distinct features of the incandescent images is not achievable,
and therefore a higher FID is not unexpected.

As it is now clear that this physical inaccuracy we accepted might influence the performance of our sampling, it is not yet clear to what extent.
To investigate this, in \autoref{fig:qualitative-evaluation-cyclegan-conditional-sampling} choose to use the same sampling method as before, but instead use the intensities produced by CycleGAN as input to the diffusion sampling.

\begin{figure}[htp!]
    \centering
    \setkeys{Gin}{width=1\linewidth}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
        NIR                                                                              & CycleGAN                                                                                   & Diffusion with CycleGAN Input                                                                    & NIR                                                                              & CycleGAN                                                                                   & Diffusion with CycleGAN Input                                                                    \\
        \includegraphics{gfx/conditional-with-cycle-gan-qual/nir_S2_B06_R1_PICT0128.jpg} & \includegraphics{gfx/conditional-with-cycle-gan-qual/cyclegan_S2_B06_R1_PICT0128_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/diff_cycle_gan_S2_B06_R1_PICT0128_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/nir_S2_B06_R1_PICT0279.jpg} & \includegraphics{gfx/conditional-with-cycle-gan-qual/cyclegan_S2_B06_R1_PICT0279_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/diff_cycle_gan_S2_B06_R1_PICT0279_fake.png} \\
        \includegraphics{gfx/conditional-with-cycle-gan-qual/nir_S2_B06_R1_PICT0387.jpg} & \includegraphics{gfx/conditional-with-cycle-gan-qual/cyclegan_S2_B06_R1_PICT0387_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/diff_cycle_gan_S2_B06_R1_PICT0387_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/nir_S2_B06_R3_PICT1364.jpg} & \includegraphics{gfx/conditional-with-cycle-gan-qual/cyclegan_S2_B06_R3_PICT1364_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/diff_cycle_gan_S2_B06_R3_PICT1364_fake.png} \\
        \includegraphics{gfx/conditional-with-cycle-gan-qual/nir_S2_B06_R3_PICT3848.jpg} & \includegraphics{gfx/conditional-with-cycle-gan-qual/cyclegan_S2_B06_R3_PICT3848_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/diff_cycle_gan_S2_B06_R3_PICT3848_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/nir_S2_B07_R1_PICT3274.jpg} & \includegraphics{gfx/conditional-with-cycle-gan-qual/cyclegan_S2_B07_R1_PICT3274_fake.png} & \includegraphics{gfx/conditional-with-cycle-gan-qual/diff_cycle_gan_S2_B07_R1_PICT3274_fake.png}
    \end{tabularx}
    \caption{
        \todo{Add caption}
    }
    \label{fig:qualitative-evaluation-cyclegan-conditional-sampling}
\end{figure}

\begin{table}[htp!]
    \centering
    \begin{tabular}{c | c}
        Model                                    & FID  $\downarrow$ \\
        \hline\hline
        CycleGAN                                 & 98.10             \\
        Diffusion with CycleGAN Intensity Inputs & \textbf{97.21}    \\
        Diffusion with NIR Inputs                & 104.04
    \end{tabular}
    \caption{
        \todo{Add caption}   
    }
    \label{fig:quantitative-evaluation-cyclegan-conditional-sampling}
\end{table}


In \autoref{fig:qualitative-evaluation-cyclegan-conditional-sampling} we see that not only is the diffusion model more than capable in colorizing these pictures,
it also exceeds CycleGAN's color choices. While the zebra of CycleGAN has an orange outline at the head where it should not, the diffusion model does not make this mistake
which results in overall more realistic images.

This subtle difference between the CycleGAN and the diffusion model samples with CycleGAN intensity inputs is also visible while comparing the images quantitatively in \autoref{fig:quantitative-evaluation-cyclegan-conditional-sampling}.
The diffusion model performs better than CycleGAN when the intensities are appropriate.
It is not expected by the diffusion model to perform better than in the unconditional setting (assuming test dataset and train dataset are divided appropriately).
Because the diffusion model in the unconditional setting only performs $1.2$ FID points better (\autoref{fig:quantitative-evaluation-unconditional-sampling}).
Therefore, we have shown that the assumption that the near-infrared intensities approximate the visual intensities well,
is the main factor explaining the difference between the quantitative results.

\subsection{High-Pass Filtering of NIR}
\label{sec:high-pass-filter-evaluation}

To improve this, we can get inspired by simple NIR-RGB enhancement methods based on filters \parencite{rgb-nir-image-enhancement}.
A better approximation is to use only the high-frequencies of the near-infrared image while the low frequencies can still be sampled by the diffusion model.
This intuitively also gives the diffusion model freedom to sample different illumination than given by the near-infrared image.

In \autoref{fig:qualitative-evaluation-full-high-pass} we evaluate the results of this concept. 

\begin{figure}[htp!]
    \centering
    \setkeys{Gin}{width=1\linewidth}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
        NIR & Full-Pass & High-Pass & NIR & Full-Pass & High-Pass \\
        \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/nir_S2_B06_R1_PICT0128.jpg} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/full-pass_S2_B06_R1_PICT0128.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/high-pass_S2_B06_R1_PICT0128.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/nir_S2_B06_R1_PICT0279.jpg} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/full-pass_S2_B06_R1_PICT0279.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/high-pass_S2_B06_R1_PICT0279.png}\\
        \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/nir_S2_B06_R1_PICT0387.jpg} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/full-pass_S2_B06_R1_PICT0387.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/high-pass_S2_B06_R1_PICT0387.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/nir_S2_B06_R3_PICT1364.jpg} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/full-pass_S2_B06_R3_PICT1364.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/high-pass_S2_B06_R3_PICT1364.png}\\
        \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/nir_S2_B06_R3_PICT3848.jpg} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/full-pass_S2_B06_R3_PICT3848.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/high-pass_S2_B06_R3_PICT3848.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/nir_S2_B07_R1_PICT3274.jpg} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/full-pass_S2_B07_R1_PICT3274.png} & \includegraphics{gfx/diffusion-sampling-full-vs-high-pass-filter-qual/high-pass_S2_B07_R1_PICT3274.png}
    \end{tabularx}
    \caption{
        \todo{Add caption}
    }
    \label{fig:qualitative-evaluation-full-high-pass}
\end{figure}

\begin{table}[htp!]
    \centering
    \begin{tabular}{c | c}
        Strategy      & FID  $\downarrow$ \\
        \hline\hline
        Full-Pass     & 105.00            \\
        High-Pass     & 95.49             \\
        Unconditional & 96.02
    \end{tabular}
    \caption{
        \todo{Add caption}
    }
    \label{fig:quantitative-evaluation-full-high-pass}
\end{table}


As expected the diffusion now using the high-pass filter, the diffusion model can manipulate the illumination for the scene.
For example, we observe for the first image of the fix that now, not only the fox itself but also the environment has an orange-lighting.
This is quite typical for the target domain, that is why the diffusion model generates such images. 
This reduction in restriction also positively influences the quantitative measurements: 
This method achieves a FID of $95.49$ which is even $0.53$ FID points better than the unconditional sampling (\autoref{fig:quantitative-evaluation-full-high-pass}).
\todo{Resolve conflict to statement: "unconditional sampling is always the best" }

\subsection{Influence of Hyperparameter Sigma}
\label{sec:influence-of-sigma-evaluation}



\section{Diffusion and GAN}
\label{sec:diffusion-vs-cyclegan}
To study the difference between diffusion models and generative adversarial networks for near-infrared image colorization, we compare them for two specialized problems.

First we evaluate the performances on the Caltech Camera Traps dataset (\autoref{sec:cct}) by \Citeauthor*{caltech}.
The near-infrared images in the dataset are mostly taken during the night, while the color images are from the daytime \parencite{caltech}.
Both networks are required to translate mostly night near-infrared images to colored images while having mostly seen day images.
This results in artificial difficulty for the networks (\autoref{sec:diffusion-vs-cyclegan-day}).

Secondly we evaluate both networks on a more application-oriented dataset.
The subset of the Serengeti dataset we use consists mostly of night images for the near infrared domain as for the colored domain.
As near-infared applications lie for the most part in generating high-quality night images this is closer to an application context.
Because the networks are not required to perform a translation to colored night images, without having seen many of those, we consider
this specialized problem as easier, which is also visible in the quality of the results (\autoref{sec:diffusion-vs-cyclegan-night})

\subsection{Extended Dataset --- Robustness}
\label{sec:diffusion-vs-cyclegan-day}
\todo{Redo with new sampling method}

Using the Caltech Camera Traps data split we introduced in \autoref{sec:cct},
we train an unconditional diffusion model on the mostly colored daytime images from the training dataset.

Our unconditional model is evaluated first to validate the model did learn to approximate the target distribution well.
As visible in \autoref{fig:qualitative-evaluation-unconditional-sampling-caltech} qualitatively the diffusion network
produces highly realistic images matching the style and diversity of the test dataset well.
Quantitatively the samples achieve a fréchet inception distance of $100.22$ to the test data set.
Compared with other FIDs in this work we consider it a good value.

\begin{figure}[htp!]
    \centering
    \setkeys{Gin}{width=1\linewidth}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
        RGB                                                                                                              & Diffusion                                                                               & RGB                                                                                                              & Diffusion                                                                               & RGB                                                                                                              & Diffusion                                                                               \\
        \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_5858c0dc-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00000.png} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_585a640e-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00001.png} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_585a6486-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00002.png} \\
        \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_585dab96-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00003.png} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_585f4d99-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00004.png} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_585f4fbd-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00005.png} \\
        \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_5860ef9d-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00006.png} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_58629181-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00007.png} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/rgb_58629415-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-caltech-qual/diffusion_00008.png}
    \end{tabularx}
    \caption{
        \todo{Add caption}
    }
    \label{fig:qualitative-evaluation-unconditional-sampling-caltech}
\end{figure}

Next our CycleGAN network is trained with images from both domains to learn a function while maintaining a cycle-consistency property according to \autoref{sec:methods-cycle-gan}.
We sample using our sampling procedure with the diffusion model given the near-infrared images of the test dataset (\todo{}) and simultaneously
sample using CycleGAN.

\begin{figure}[htp!]
    \centering
    \setkeys{Gin}{width=1\linewidth}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
        NIR                                                                                                            & CycleGAN                                                                                                                 & Diffusion                                                                                                            & NIR                                                                                                            & CycleGAN                                                                                                                 & Diffusion                                                                                                            \\
        \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/nir_585a6303-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/cyclegan_585a6303-23d2-11e8-a6a3-ec086b02610b_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/diffusion_585a6303-23d2-11e8-a6a3-ec086b02610b.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/nir_585a6394-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/cyclegan_585a6394-23d2-11e8-a6a3-ec086b02610b_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/diffusion_585a6394-23d2-11e8-a6a3-ec086b02610b.png} \\
        \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/nir_585c042f-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/cyclegan_585c042f-23d2-11e8-a6a3-ec086b02610b_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/diffusion_585c042f-23d2-11e8-a6a3-ec086b02610b.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/nir_585c05fe-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/cyclegan_585c05fe-23d2-11e8-a6a3-ec086b02610b_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/diffusion_585c05fe-23d2-11e8-a6a3-ec086b02610b.png} \\
        \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/nir_5860ede3-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/cyclegan_5860ede3-23d2-11e8-a6a3-ec086b02610b_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/diffusion_5860ede3-23d2-11e8-a6a3-ec086b02610b.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/nir_586437e6-23d2-11e8-a6a3-ec086b02610b.jpg} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/cyclegan_586437e6-23d2-11e8-a6a3-ec086b02610b_fake.png} & \includegraphics{gfx/conditional-diffusion-sampling-caltech-qual/diffusion_586437e6-23d2-11e8-a6a3-ec086b02610b.png}
    \end{tabularx}
    \caption{
        \todo{Add caption}
    }
    \label{fig:qualitative-evaluation-conditional-sampling-caltech}
\end{figure}

\begin{table}[htp!]
    \centering
    \begin{tabular}{c | c}
        Model                   & FID  $\downarrow$ \\
        \hline\hline
        CycleGAN                & 114.21            \\
        Unconditional Diffusion & 153.63
    \end{tabular}
    \caption{
        \todo{Add caption}
    }
    \label{fig:quantitative-evaluation-conditional-sampling-caltech}
\end{table}

While neither CycleGAN nor the diffusion model perform great given this problem, we can use this problem to analyze both strengths and weaknesses.
We observe that CycleGAN hallucinates much.
Many artifacts such as green patterns where otherwise darkness would be expected are created (\autoref{fig:qualitative-evaluation-conditional-sampling-caltech}).
It seems that CycleGAN produces features which are common for day images without validating they exist in the input.
For the human eye such artifacts make the images less realistic. Additionally, the faithfulness and content preservation is also reduced.

On the other the diffusion model does not create such hallucinations and is generally faithful to the input image by design.
Its samples appear rather monochrome than colored (\autoref{fig:qualitative-evaluation-conditional-sampling-caltech}).
When comparing with the unconditional sampling, this indicates the diffusion model learned to approximate the target domain in daytime areas well, but is unfamiliar with nighttime images.
The restriction of the diffusion model sampling to keep the intensities of the near-infared image forces the model to stay in unfamiliar areas of the high-dimensional space.
This leads to less colorization by the model.

When comparing the fréchet inception distance, CycleGAN performs much better than the diffusion model having a difference of 39.42 points (\autoref{fig:quantitative-evaluation-conditional-sampling-caltech}).
The FID calculates the difference between the test dataset's colored images and those produced by our models.
The test dataset's colored images are mostly day images.
Therefore, the superiority of the CycleGAN considering the FID is not surprising.
Unrealistic and unfaithful artifacts discovered in the qualitative evaluation lead to a higher similarity to day images from the test dataset.
The diffusion model on the other hand produces less colored night images, which results to a higher distance between the test dataset.
In this case the FID appears to be a metric not well suited for this evaluation, wee further discussion this in \autoref{sec:evaluate-fid} and suggest improvements in \autoref{sec:future-work}.

\subsection{General Comparison}
\label{sec:diffusion-vs-cyclegan-night}
Finally, we study the diffusion models performance in comparison to CycleGAN.
An unconditional diffusion model for a resolution of $128 \times 128$ was trained on the RGB domain of the same training dataset as CycleGAN.

First we evaluate the unconditional sampling of the diffusion model and therefore validate \parencite{diffusion-beats-gans}'s results for image synthesis in still hold for this dataset:

\begin{table}[htp!]
    \centering
    \begin{tabular}{c | c}
        Model                   & FID  $\downarrow$ \\
        \hline\hline
        CycleGAN                & 98.10             \\
        Unconditional Diffusion & 95.73
    \end{tabular}
    \caption{
        \textbf{Quantitative Evaluation.} CycleGAN and the diffusion model trained on Snapshot Serengeti \parencite{serengeti} containing only night NIR and RGB images.
        Comparing the FID calculated between the test dataset and the generated images.
    }
    \label{fig:quantitative-evaluation-unconditional-sampling}
\end{table}

\begin{figure}[htp!]
    \centering
    \todo{Better choice of images}
    \setkeys{Gin}{width=1\linewidth}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
        RGB                                                                                    & CycleGAN                                                                                         & Diffusion                                                                                    & RGB                                                                                    & CycleGAN                                                                                         & Diffusion                                                                                    \\
        \includegraphics{gfx/unconditional-diffusion-sampling-qual/rgb_S2_B04_R3_IMAG0432.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/cyclegan_S2_B06_R1_PICT0128_fake.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/diffusion_S2_B06_R1_PICT0128.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/rgb_S2_B04_R3_IMAG0471.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/cyclegan_S2_B06_R1_PICT0279_fake.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/diffusion_S2_B06_R1_PICT0279.png} \\
        \includegraphics{gfx/unconditional-diffusion-sampling-qual/rgb_S2_B05_R1_IMAG0084.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/cyclegan_S2_B06_R1_PICT0387_fake.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/diffusion_S2_B06_R1_PICT0387.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/rgb_S2_B05_R1_IMAG0132.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/cyclegan_S2_B06_R3_PICT1364_fake.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/diffusion_S2_B06_R3_PICT1364.png} \\
        \includegraphics{gfx/unconditional-diffusion-sampling-qual/rgb_S2_B05_R2_IMAG0016.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/cyclegan_S2_B06_R3_PICT3848_fake.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/diffusion_S2_B06_R3_PICT3848.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/rgb_S2_B05_R3_IMAG1018.jpg} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/cyclegan_S2_B07_R1_PICT3274_fake.png} & \includegraphics{gfx/unconditional-diffusion-sampling-qual/diffusion_S2_B07_R1_PICT3274.png}
    \end{tabularx}
    \caption{
        \textbf{Qualitative Evaluation.} Left to right: Sample image from the RGB domain of the Serengeti test dataset \parencite{serengeti},
        sample produced by CycleGAN \parencite{mehri} and an unconditional sample produced by the diffusion model \parencite{diffusion-beats-gans}.
    }
    \label{fig:qualitative-evaluation-unconditional-sampling}
\end{figure}

Qualitatively we observe in \autoref{fig:qualitative-evaluation-unconditional-sampling} that the diffusion model is capable to create realistic images.
The model produced many similar images which do not contain animals, which is not unexpectable since such exist in the training dataset as well.
In cases of animal presence it performances exceptionally well in creating realistic animals with matching lighting.
Only in few cases where trees are generated, branches are not well distinguishing, this is the only flaw which can be observed.

Quantitatively in \autoref{fig:quantitative-evaluation-unconditional-sampling} we see, the diffusion network only slightly outperforms CycleGAN's performance.
This may be caused by the usage of a smaller U-Net generator, less training iterations and a smaller dataset compared to \parencite{diffusion-beats-gans} which are all trade-off which
had to be made due to a lack of ultra-high performant computational resources.
Nevertheless, it can be argued that this unconditional diffusion model is capable enough to be a basis for a good colorization.


\begin{table}[htp!]
    \centering
    \begin{tabular}{c | c}
        Model                 & FID  $\downarrow$ \\
        \hline\hline
        CycleGAN              & 98.10             \\
        Conditional Diffusion & 103.71
    \end{tabular}
    \caption{
        \textbf{Quantitative Evaluation.} CycleGAN and the diffusion model trained on Snapshot Serengeti \parencite{serengeti} containing only night NIR and RGB images.
        Comparing the FID calculated between the test dataset and the generated images.
    }
    \label{fig:quantitative-evaluation-conditiona-sampling}
\end{table}

We compare performances of our trained CycleGAN with our conditional diffusion network.
In this case strong guidance with the NIR $\approx$ Intensity approximation is used.
\autoref{fig:quantitative-evaluation-conditiona-sampling} indicates strong performance of the conditional
diffusion network only, $\sim 6$ FID points worse than CycleGAN.
Considering the flaws analyzed and observed in \autoref{sec:evaluate-fid} those points are only a small difference.


We analyze this concrete weakness of this sampling technique for colorization further.